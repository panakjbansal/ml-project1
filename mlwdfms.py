# -*- coding: utf-8 -*-
"""mlwdfms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N15D4xF-jco8crrXIclcUXE0CbktifZ6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler

column=["fLength","fWidth","fSize","fConc","fConc1","fAsym","fM3Long","fM3Trans","fAlpha","fDist","class"]
df=pd.read_csv('magic04.data',names=column)
df.head()

#df['class'].unique()
#len(df['class'])
#print(df['class'])
#lt=[]
#for i in range(0,len(df['class'])-1):
#  if(df['class'][i]=='g'):
#    df['class'][i]=1
#  else:
#    df['class'][i]=0
#df['class'][19019]=0

df['class']=(df['class']=="g").astype(int)

df

#plotting the graph to see the distribution of the data
for label in column[:-1]:
  plt.hist(df[df['class']==1][label],color='red',alpha=0.5,label='gamma' , density=True)
  plt.hist(df[df['class']==0][label],color='blue',alpha=0.5, label='hadron',density=True)
  plt.title(label)
  plt.legend()
  plt.ylabel('Probability')
  plt.xlabel(label)
  plt.show()

# Dividing the data into training-data ,validation-data and testing-data
# Below df.sample is to shuffle the data  adn the divisioin is like 0 to 60percent is training data and 60 to 80 percent is validation data and rest is testing data
train,valid,test=np.split(df.sample(frac=1),[int(0.6*len(df)),int(0.8*len(df))])

# Now we will scale the data set as we can see the data in dataframes between different columns vary a lot so we will bring them to particular range .
# Now  we know that the length of the dataset of gamma and hydron they will vary a lot so we will have to increse hydron values in sampleset

def scaledataset(dataframe, oversample=False):
  x=dataframe[dataframe.columns[:-1]].values
  y=dataframe[dataframe.columns[-1]].values
  scaler=StandardScaler()
  rot=RandomOverSampler()
  x=scaler.fit_transform(x)
  if oversample:
    x,y=rot.fit_resample(x,y)

  data =np.hstack((x,np.reshape(y,(-1,1))))
  return data,x, y

# Now  we know that the length of the dataset of gamma and hydron they will vary a lot so we will have to increse hydron values in sampleset
train,x_train,y_train=scaledataset(train,oversample=True)
valid,x_valid,y_valid=scaledataset(valid,oversample=False)
test,x_test,y_test=scaledataset(test,oversample=False)
#print(type(df['class'][0]))

"""# **KNN**"""

from sklearn.neighbors import KNeighborsClassifier
#from sklearn.metrics import Classification_report
from sklearn.metrics import classification_report

knn=KNeighborsClassifier(n_neighbors=1)
knn.fit(x_train,y_train)

y_pred=knn.predict(x_test)
#Generating th eclassification report
print(classification_report(y_test,y_pred))

"""## **NAIVE BAYES MODEL**"""

from sklearn.naive_bayes import GaussianNB
nb=GaussianNB()
nb.fit(x_train, y_train)
y_pred=nb.predict(x_test)

print(classification_report(y_test,y_pred))

"""# **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

lg_model=LogisticRegression()
lg_model.fit(x_train,y_train)
y_pred=lg_model.predict(x_test)
print(classification_report(y_test,y_pred))

"""# **SVM(Support Vector Machine)**"""

from sklearn.svm import SVC
svm_model=SVC()
svm_model.fit(x_train,y_train)
y_predict=svm_model.predict(x_test)
print(classification_report(y_test,y_predict))

"""# **Neural Network**"""

import tensorflow as tf
def trainmodel(x_train,y_train,numnodes,dropout_prob,lr,batch_size,epochs):
  nn_model =tf.keras.Sequential(
      [tf.keras.layers.Dense(numnodes,activation='relu',input_shape=(10,)),
       tf.keras.layers.Dropout(dropout_prob),
       tf.keras.layers.Dense(numnodes,activation='relu'),
       tf.keras.layers.Dropout(dropout_prob),
       tf.keras.layers.Dense(1,activation='sigmoid')]
  )
  nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss="binary_crossentropy",metrics=['Accuracy'])
  history=nn_model.fit(x_train,y_train,epochs =epochs,batch_size=numnodes,validation_split=0.2)
  return nn_model,history

def plotloss(history):
  fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))
  ax1.plot(history.history['loss'],label='loss')
  ax1.plot(history.history['val_loss'],label='val_loss')
  ax1.set_xlabel('Epoch')#epoch means training cycle
  ax1.set_ylabel('Binary Crossentropy')
  ax1.legend()
  ax1.grid(True)

  ax2.plot(history.history['Accuracy'],label='accuracy')
  ax2.plot(history.history['val_Accuracy'],label='val_accuracy')
  ax2.set_xlabel('Epoch')#epoch means training cycle
  ax2.set_ylabel('Binary Crossentropy')
  ax2.legend()
  ax2.grid(True)

  plt.show()

def plotaccuracy(history):
  plt.plot(history.history['Accuracy'],label='accuracy')
  plt.plot(history.history['val_Accuracy'],label='val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.grid(True)
  plt.show()

#history=nn_model.fit(x_train,y_train,epochs =100,batch_size=32,validation_split=0.2)

least_val_loss=float('inf')
print(type(least_val_loss))
least_loss_model=None
epochs=1000
for num_nodes in [16,32,64]:
  for dropout_prob in [0,0.2]:#dropout prob means probability whose value is between 0 and 1
    for lr in [0.01,0.05,0.001]:
      for batch_size in [32,64,128]:
        model,history=trainmodel(x_train,y_train,num_nodes,dropout_prob,lr,batch_size,epochs)
        plotloss(history)
        #plotaccuracy(history)
        val_loss=model.evaluate(x_valid,y_valid)
        print(val_loss)
        print(model.metrics_names)
        g=float(val_loss[0])
        if g < least_val_loss:
          least_val_loss=val_loss[0]
          least_loss_model=model#so this model will be the bst model for this type of data among all the tried models